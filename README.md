# ğŸ”µ Blind Navigation Hybrid AI System
### Real-time AI Guidance System for Visually Impaired Users

This project implements a **real-time blind navigation system** that detects walkable paths, obstacles, potholes, stairs, vehicles, and environmental hazards using a hybrid Computer Vision pipeline. It continuously guides visually impaired users via audio commands.

The system runs locally using a webcam and is optimized to work on mid-range hardware (e.g., **Core i5 6th Gen + 16GB RAM**).

**Example Audio Commands:**
> - â€œMove slightly leftâ€
> - â€œPothole ahead, stop!â€
> - â€œStairs detectedâ€
> - â€œWalk forward safelyâ€

---

## ğŸš€ Features

* **Hybrid Vision Pipeline:** Combines pretrained YOLOv8 with custom-trained classes for domain-specific objects (road, footpath, pothole, wet floor, stairs, bricks, obstacles, construction objects, signboards).
* **Depth Estimation:** Uses MiDaS / ZoeDepth for monocular distance estimation to warn users of proximity to obstacles.
* **Walkable-area Segmentation:** Highlights road/walkable areas to calculate the safest path.
* **GPS Navigation (Optional):** Accepts a destination and provides turn-by-turn guidance merged with camera-based obstacle avoidance.
* **Voice Interaction:**
    * **Input:** Speech-to-Text (Vosk/Whisper) for user commands.
    * **Output:** Text-to-Speech (pyttsx3/Coqui) for real-time guidance.
* **Logic Engine:** Merges object detections, depth data, and route info to produce timely, prioritized audio instructions.

---

## ğŸ“‚ Project Structure

```text
blindnav_hybrid/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ setup.bat / setup.sh
â”œâ”€â”€ LICENSE
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â””â”€â”€ val/
â”‚   â”œâ”€â”€ labels/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â””â”€â”€ val/
â”‚   â””â”€â”€ annotations/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ pretrained/
â”‚   â””â”€â”€ custom/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ camera_stream.py
â”‚   â”œâ”€â”€ object_detection.py
â”‚   â”œâ”€â”€ custom_train.py
â”‚   â”œâ”€â”€ depth_estimation.py
â”‚   â”œâ”€â”€ segmentation.py
â”‚   â”œâ”€â”€ gps_navigation.py
â”‚   â”œâ”€â”€ speech_to_text.py
â”‚   â”œâ”€â”€ text_to_speech.py
â”‚   â”œâ”€â”€ logic_engine.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ viz.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ collect_images.py
â”‚   â”œâ”€â”€ annotate_helper.py
â”‚   â””â”€â”€ convert_annotations.py
â””â”€â”€ notebooks/





---

## ğŸ§ª Installation & Setup (Windows)

1. Clone the repo:
```bash
git clone https://github.com/YOUR_USERNAME/Blind-Human-Navigation-Hybrid-AI-System.git
cd Blind-Human-Navigation-Hybrid-AI-System

## Setup Instructions

### 1. Create & Activate Virtual Environment

```bash
python -m venv .venv
# On Windows
.venv\Scripts\activate
# On Linux/Mac
source .venv/bin/activate

### 2. Upgrade pip & Install Dependencies
python -m pip install --upgrade pip
pip install -r requirements.txt

### 3.(Optional) If on Linux/Mac, run setup script
bash setup.sh


### 4. Place Pretrained Weights
Make sure your pretrained weights are in the following folder structure:
models/pretrained/
  â””â”€â”€ yolov8n.pt   (or other yolov8 weights)

## Train Custom Classes
### 1.Prepare YOLO-format dataset:
data/images/train/
data/images/val/
data/labels/train/
data/labels/val/

Each image must have a .txt label file (YOLO format):
<class_id> <x_center> <y_center> <width> <height> (normalized)

### 2.Create data.yaml with paths and class names. Example:
